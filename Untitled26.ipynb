{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMh-LSg_OaOc"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Wybór odpowiedniego kodera dla modelu GPT-4\n",
        "encoding = tiktoken.get_encoding(\"gpt-4\")\n",
        "\n",
        "# Przykładowy tekst\n",
        "text = \"Jak policzyć ilość tokenów w zapytaniu?\"\n",
        "\n",
        "# Tokenizacja tekstu\n",
        "tokens = encoding.encode(text)\n",
        "\n",
        "# Liczba tokenów\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(f\"Liczba tokenów: {num_tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Twoje API key\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "# Przykładowy tekst\n",
        "text = \"Jak policzyć ilość tokenów w zapytaniu?\"\n",
        "\n",
        "# Użycie endpointu do obliczenia tokenów\n",
        "response = openai.Completion.create(\n",
        "  engine=\"gpt-4\",\n",
        "  prompt=text,\n",
        "  max_tokens=1  # Możemy ustawić małą wartość, ponieważ nie chcemy generować nowego tekstu, tylko obliczyć tokeny\n",
        ")\n",
        "\n",
        "# Liczba tokenów w odpowiedzi\n",
        "num_tokens = response['usage']['total_tokens']\n",
        "\n",
        "print(f\"Liczba tokenów: {num_tokens}\")\n"
      ],
      "metadata": {
        "id": "lZDlPgV1OekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvangN3UOenT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if len(current_chunk) + len(word) + 1 > max_length:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "        current_chunk.append(word)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "DoRNu2-kOeql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "def summarize_chunk(chunk):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Podsumuj ten tekst:\\n\\n{chunk}\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "FrnfOidhOeuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, max_chunk_length):\n",
        "    chunks = split_text(text, max_chunk_length)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summary = ' '.join(summaries)\n",
        "    return combined_summary\n"
      ],
      "metadata": {
        "id": "XemPYFMWOu_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykładowy bardzo długi tekst\n",
        "long_text = \"Twoj bardzo długi tekst tutaj...\"\n",
        "\n",
        "# Maksymalna długość fragmentu (dopasuj według limitu tokenów modelu)\n",
        "max_chunk_length = 1000  # przykładowa długość w słowach\n",
        "\n",
        "# Podsumowanie całego tekstu\n",
        "final_summary = summarize_text(long_text, max_chunk_length)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "14mIxvYuOvDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quQEHTYLOvF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_by_chars(text, max_length):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max_length):\n",
        "        chunks.append(text[i:i + max_length])\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "dZ2XxmNHOvIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "def summarize_chunk(chunk):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Podsumuj ten tekst:\\n\\n{chunk}\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "j0KNIJtaOvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, max_chunk_length):\n",
        "    chunks = split_text_by_chars(text, max_chunk_length)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summary = ' '.join(summaries)\n",
        "    return combined_summary\n"
      ],
      "metadata": {
        "id": "fyx7QwBkPDEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykładowy bardzo długi tekst\n",
        "long_text = \"Twój bardzo długi tekst tutaj...\"\n",
        "\n",
        "# Maksymalna długość fragmentu w znakach (dopasuj według limitu tokenów modelu)\n",
        "max_chunk_length = 2000  # przykładowa długość w znakach\n",
        "\n",
        "# Podsumowanie całego tekstu\n",
        "final_summary = summarize_text(long_text, max_chunk_length)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "V4vNMsVEPDHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgMkv3MZPDRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}