{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1045f785",
      "metadata": {
        "Collapsed": "false",
        "id": "1045f785"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "HOME = os.environ.get(\"HOME\")\n",
        "CAFILE = HOME + \"/cacert.pem\"\n",
        "API = \"https://patagonia-aigen-srv:8080/llm-large/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35337d3",
      "metadata": {
        "Collapsed": "false",
        "id": "e35337d3"
      },
      "outputs": [],
      "source": [
        "def api_config(api):\n",
        "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
        "    openai_api_key = \"EMPTY\"\n",
        "    openai_api_base = api\n",
        "    os.environ[\"SSL_CERT_FILE\"] = CAFILE\n",
        "\n",
        "    print(\"[INFO] API:\" , openai_api_base)\n",
        "\n",
        "    client = OpenAI(\n",
        "        api_key=openai_api_key,\n",
        "        base_url=openai_api_base,\n",
        "    )\n",
        "\n",
        "    # Checking availlable model\n",
        "    models = client.models.list()\n",
        "    model = models.data[0].id\n",
        "    print(models)\n",
        "    print(\"[INFO] Model:\" , model,\"\\n\")\n",
        "    return client, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a2828b",
      "metadata": {
        "Collapsed": "false",
        "id": "c8a2828b",
        "outputId": "f125cc7c-fc51-40e1-cd67-b69b45625697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] API: https://patagonia-aigen-srv:8080/llm-large/v1\n",
            "SyncPage[Model](data=[Model(id='/app/models/mistralai/Mixtral-8x7B-Instruct-v0.1', created=1721208125, object='model', owned_by='vllm', root='/app/models/mistralai/Mixtral-8x7B-Instruct-v0.1', parent=None, permission=[{'id': 'modelperm-774fd77d41fe439eb49444f7365649c0', 'object': 'model_permission', 'created': 1721208125, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')\n",
            "[INFO] Model: /app/models/mistralai/Mixtral-8x7B-Instruct-v0.1 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "client, model = api_config(API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f123dfed",
      "metadata": {
        "Collapsed": "false",
        "id": "f123dfed"
      },
      "outputs": [],
      "source": [
        "history = [{\"role\": \"user\", \"content\": \"siema\"}]\n",
        "response = client.chat.completions.create(\n",
        "    messages=history,\n",
        "    model=model,\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d590d08b",
      "metadata": {
        "Collapsed": "false",
        "id": "d590d08b",
        "outputId": "0b4014d6-ba82-4bb5-b457-7808cd670f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hello! It seems like you've typed \"siema\" which might be a typo or a name. If you're looking for a conversation, I'm here to help!\n",
            "\n",
            "I can assist you with any questions you have or engage in a discussion on a topic of your choice. Some popular topics include technology, science, current events, and pop culture.\n",
            "\n",
            "If you need help with something specific, please provide more context or clarify your question. I'm here to make your experience as smooth as possible.\n",
            "\n",
            "Let's have a great conversation!\n"
          ]
        }
      ],
      "source": [
        "for choice in response.choices:\n",
        "    message = choice.message\n",
        "    if message.role == 'assistant':\n",
        "        print(message.content)\n",
        "#     print(message.__dict__)\n",
        "#     print(message.content)\n",
        "#     print(message.role)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a32221",
      "metadata": {
        "Collapsed": "false",
        "id": "f7a32221"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(txt_filename):\n",
        "    with open(txt_filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return content\n",
        "# text = read_txt_file('./Transkrypcja__Gildia_AI_Tech_2024.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75bb9ebe",
      "metadata": {
        "Collapsed": "false",
        "id": "75bb9ebe"
      },
      "outputs": [],
      "source": [
        "text = read_txt_file(\"./Transkrypcja__Gildia_AI_Tech_2024.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ddb2c8",
      "metadata": {
        "Collapsed": "false",
        "id": "36ddb2c8"
      },
      "outputs": [],
      "source": [
        "# history = [\n",
        "#     {\"role\": \"user\", \"content\": f\"Proszę znajdź następujące informacje z dostarczonego tekstu:\\n{text}\\n\" +\n",
        "#                                \"\\n\".join([f\"{k}: {v}\" for k, v in data_explanation.items()])}\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c56ad98",
      "metadata": {
        "Collapsed": "false",
        "id": "6c56ad98"
      },
      "outputs": [],
      "source": [
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0811bd29",
      "metadata": {
        "Collapsed": "false",
        "id": "0811bd29"
      },
      "outputs": [],
      "source": [
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"\"\"\n",
        "    Please summarize the main points discussed in the following text in Polish:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    history = [\n",
        "    {\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=history,\n",
        "        model=model,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9545fff0",
      "metadata": {
        "Collapsed": "false",
        "id": "9545fff0"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fea89e",
      "metadata": {
        "Collapsed": "false",
        "id": "71fea89e"
      },
      "outputs": [],
      "source": [
        "text_1 = read_txt_file('./Transkrypcja__Gildia_AI_Tech_2024.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1146cd57",
      "metadata": {
        "Collapsed": "false",
        "id": "1146cd57",
        "outputId": "a627b23b-7897-43d9-806f-45e81bc05cf7"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Choice' object has no attribute 'text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_summary)\n",
            "Cell \u001b[0;32mIn [29], line 3\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m split_text(text)\n\u001b[0;32m----> 3\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m [summarize_chunk(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m      4\u001b[0m     combined_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(summaries)\n\u001b[1;32m      5\u001b[0m     final_summary \u001b[38;5;241m=\u001b[39m summarize_chunk(combined_summaries)\n",
            "Cell \u001b[0;32mIn [29], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m split_text(text)\n\u001b[0;32m----> 3\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m [\u001b[43msummarize_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m      4\u001b[0m     combined_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(summaries)\n\u001b[1;32m      5\u001b[0m     final_summary \u001b[38;5;241m=\u001b[39m summarize_chunk(combined_summaries)\n",
            "Cell \u001b[0;32mIn [28], line 18\u001b[0m, in \u001b[0;36msummarize_chunk\u001b[0;34m(chunk)\u001b[0m\n\u001b[1;32m      6\u001b[0m history \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m     messages\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
            "File \u001b[0;32m~/envs/ailab-eda-env/lib/python3.8/site-packages/pydantic/main.py:828\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Choice' object has no attribute 'text'"
          ]
        }
      ],
      "source": [
        "final_summary = summarize_text(text_1)\n",
        "print(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9188c4",
      "metadata": {
        "Collapsed": "false",
        "id": "db9188c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2ca464f9",
      "metadata": {
        "Collapsed": "false",
        "id": "2ca464f9"
      },
      "source": [
        "### proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0f2fdb",
      "metadata": {
        "Collapsed": "false",
        "id": "2a0f2fdb"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef08592",
      "metadata": {
        "Collapsed": "false",
        "id": "4ef08592",
        "outputId": "c3eb1ac6-200d-408a-a514-2c3ba254f903"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;43;03m#     model = 'text-embedding-ada-002',\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/envs/ailab-eda-env/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     emit_warning()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/envs/ailab-eda-env/lib/python3.8/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "embed = OpenAIEmbeddings(\n",
        "#     model = 'text-embedding-ada-002',\n",
        "    model=model)\n",
        "#     openai_api_key = \"EMPTY\"\n",
        "#     openai_api_base = api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b92f6e6",
      "metadata": {
        "Collapsed": "false",
        "id": "0b92f6e6",
        "outputId": "818aa224-10d6-4e9f-e85a-94d799a1749c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load a single document using LangChain's PDF Loader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(\u001b[43mfile\u001b[49m)\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
          ]
        }
      ],
      "source": [
        "# Load a single document using LangChain's PDF Loader\n",
        "loader = PyPDFLoader(file)\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa515ded",
      "metadata": {
        "Collapsed": "false",
        "id": "aa515ded"
      },
      "outputs": [],
      "source": [
        "# split documents into text and embeddings\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "   chunk_size=1000,\n",
        "   chunk_overlap=150\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74994ad6",
      "metadata": {
        "Collapsed": "false",
        "id": "74994ad6",
        "outputId": "15f0e9f7-1fcb-478a-e476-7782c22566e9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'chunks' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create an array of text to embed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m context_array \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mchunks\u001b[49m):\n\u001b[1;32m      4\u001b[0m   context_array\u001b[38;5;241m.\u001b[39mappend(row\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Submit array to OpenAI, which will return a list of embeddings \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# calculated from the input array of text chunks\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
          ]
        }
      ],
      "source": [
        "# Create an array of text to embed\n",
        "context_array = []\n",
        "for i, row in enumerate(chunks):\n",
        "  context_array.append(row.page_content)\n",
        "\n",
        "# Submit array to OpenAI, which will return a list of embeddings\n",
        "# calculated from the input array of text chunks\n",
        "emb_vectors = embed.embed_documents(context_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f133d0e",
      "metadata": {
        "Collapsed": "false",
        "id": "0f133d0e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b77d9c3",
      "metadata": {
        "Collapsed": "false",
        "id": "8b77d9c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Set environment variables and constants\n",
        "HOME = os.environ.get(\"HOME\")\n",
        "CAFILE = HOME + \"/cacert.pem\"\n",
        "API = \"https://patagonia-aigen-srv:8080/llm-large/v1\"\n",
        "CHUNK_SIZE = 1024  # Define the chunk size based on the model's maximum token limit\n",
        "\n",
        "def api_config(api):\n",
        "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
        "    openai.api_key = \"EMPTY\"\n",
        "    openai.api_base = api\n",
        "    os.environ[\"SSL_CERT_FILE\"] = CAFILE\n",
        "\n",
        "    print(\"[INFO] API:\", openai.api_base)\n",
        "\n",
        "    # Checking available models\n",
        "    models = openai.Model.list()\n",
        "    model = models['data'][0]['id']\n",
        "    print(models)\n",
        "    print(\"[INFO] Model:\", model, \"\\n\")\n",
        "    return model\n",
        "\n",
        "# Configure API and get the model\n",
        "model = api_config(API)\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(history):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=history,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Function to read and split the text into chunks\n",
        "def read_and_split_text(file_path, chunk_size):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Function to summarize the text chunks\n",
        "def summarize_text_chunks(chunks):\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        history = [{\"role\": \"user\", \"content\": f\"Proszę podsumuj następujący tekst:\\n{chunk}\"}]\n",
        "        summary = generate_response(history)\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "# Function to combine and refine summaries\n",
        "def refine_summary(summaries):\n",
        "    combined_summary = \" \".join(summaries)\n",
        "    history = [{\"role\": \"user\", \"content\": f\"Proszę podsumuj następujące podsumowania:\\n{combined_summary}\"}]\n",
        "    refined_summary = generate_response(history)\n",
        "    return refined_summary\n",
        "\n",
        "# Main function to handle the summarization process\n",
        "def summarize_long_text(file_path):\n",
        "    chunks = read_and_split_text(file_path, CHUNK_SIZE)\n",
        "    summaries = summarize_text_chunks(chunks)\n",
        "    final_summary = refine_summary(summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Example usage\n",
        "file_path = \"path_to_your_file.txt\"\n",
        "final_summary = summarize_long_text(file_path)\n",
        "print(\"Final Summary:\")\n",
        "print(final_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Set environment variables and constants\n",
        "HOME = os.environ.get(\"HOME\")\n",
        "CAFILE = HOME + \"/cacert.pem\"\n",
        "API = \"https://patagonia-aigen-srv:8080/llm-large/v1\"\n",
        "CHUNK_SIZE = 1024  # Define the chunk size based on the model's maximum token limit\n",
        "\n",
        "def api_config(api):\n",
        "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
        "    openai.api_key = \"EMPTY\"\n",
        "    openai.api_base = api\n",
        "    os.environ[\"SSL_CERT_FILE\"] = CAFILE\n",
        "\n",
        "    print(\"[INFO] API:\", openai.api_base)\n",
        "\n",
        "    # Checking available models\n",
        "    models = openai.Model.list()\n",
        "    model = models['data'][0]['id']\n",
        "    print(models)\n",
        "    print(\"[INFO] Model:\", model, \"\\n\")\n",
        "    return model\n",
        "\n",
        "# Configure API and get the model\n",
        "model = api_config(API)\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(history):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=history,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text(text, chunk_size=CHUNK_SIZE):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Function to summarize a chunk of text\n",
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"Proszę podsumuj następujący tekst:\\n{chunk}\"\n",
        "    history = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    summary = generate_response(history)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize the entire text\n",
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Example usage\n",
        "file_path = \"path_to_your_file.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "final_summary = summarize_text(text)\n",
        "print(\"Final Summary:\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "UlxzP9wZPdIG"
      },
      "id": "UlxzP9wZPdIG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ailab-eda-env",
      "language": "python",
      "name": "ailab-eda-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}