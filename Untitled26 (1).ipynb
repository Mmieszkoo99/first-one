{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "iiy0Ev_kqRBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"\"\"\n",
        "    Please summarize the main points discussed in the following text in Polish:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.Completion.create(\n",
        "        engine=\"mistral-7b\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "XIe47ha3qQ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Przykład użycia\n",
        "text = \"Twój bardzo długi tekst...\"\n",
        "final_summary = summarize_text(text)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "vQ4Ahh40qQ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "HOME = os.environ.get(\"HOME\")\n",
        "CAFILE = HOME + \"/cacert.pem\"\n",
        "API = \"https://patagonia-aigen-srv:8080/llm-large/v1\"\n",
        "\n",
        "# Konfiguracja klienta OpenAI\n",
        "openai.api_base = API\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.verify_ssl_certs = True\n",
        "openai.ca_certs = CAFILE\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"\"\"\n",
        "    Please summarize the main points discussed in the following text in Polish:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.Completion.create(\n",
        "        engine=\"mistral-7b\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n",
        "\n",
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Przykład użycia\n",
        "text = \"Twój bardzo długi tekst...\"\n",
        "final_summary = summarize_text(text)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "p61mjxANqQye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector embeddings"
      ],
      "metadata": {
        "id": "VM15yJsgujcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "XfkoznG_qQwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "ozpI_LdfqQuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "zNF5F-bAqQr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "V-KffzbaqQpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks):\n",
        "    embeddings = embed_model.encode(chunks, convert_to_tensor=True)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "Wtw0qhmuu2rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_embeddings(embeddings):\n",
        "    summary_vector = embeddings.mean(axis=0)\n",
        "    return summary_vector\n"
      ],
      "metadata": {
        "id": "LkYLZemiu3fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YZUZ71fu5iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"Please summarize the main points discussed in the following text in Polish:\\n{chunk}\\n\"\n",
        "\n",
        "    response = client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "Fyr4vFN9u5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    embeddings = embed_chunks(chunks)\n",
        "    preliminary_summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(preliminary_summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Przykład użycia\n",
        "text = \"Twój bardzo długi tekst...\"\n",
        "final_summary = summarize_text(text)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "mA3BtF2Vu-AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral 7b do osadzania wektorów?"
      ],
      "metadata": {
        "id": "9xiiMjKyvZrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(text):\n",
        "    prompt = f\"Generate embeddings for the following text:\\n{text}\\n\"\n",
        "\n",
        "    response = client.Embeddings.create(\n",
        "        model=model,\n",
        "        input=prompt,\n",
        "    )\n",
        "\n",
        "    embeddings = response['data'][0]['embedding']\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "SB8QUQI2vYaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "Gyn65UJlvYfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks):\n",
        "    embeddings = [get_embeddings(chunk) for chunk in chunks]\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "mBnK-cD-vYhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def summarize_embeddings(embeddings):\n",
        "    summary_vector = np.mean(embeddings, axis=0)\n",
        "    return summary_vector\n"
      ],
      "metadata": {
        "id": "G9N91VS6vYlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunk(chunk):\n",
        "    prompt = f\"Please summarize the main points discussed in the following text in Polish:\\n{chunk}\\n\"\n",
        "\n",
        "    response = client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "cPvkDVdyvnKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "    chunks = split_text(text)\n",
        "    embeddings = embed_chunks(chunks)\n",
        "    preliminary_summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
        "    combined_summaries = \" \".join(preliminary_summaries)\n",
        "    final_summary = summarize_chunk(combined_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Przykład użycia\n",
        "text = \"Twój bardzo długi tekst...\"\n",
        "final_summary = summarize_text(text)\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "5EdoVKXgvnHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wektory osadzone"
      ],
      "metadata": {
        "id": "vyrkZZYuzNZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "CnQ_P_gwvnEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"This is a document about machine learning.\",\n",
        "    \"Another document related to artificial intelligence.\",\n",
        "    \"More text about data science and machine learning.\",\n",
        "    \"Different topic about natural language processing.\",\n",
        "    \"Last document about deep learning.\"\n",
        "]\n",
        "\n",
        "# Generowanie osadzonych wektorów dla dokumentów\n",
        "embeddings = model.encode(documents)\n"
      ],
      "metadata": {
        "id": "Lzln7LjWvnCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "QNSY6GApvnAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych dokumentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: {documents[idx]} (Distance: {distances[0][i]})\")\n"
      ],
      "metadata": {
        "id": "V3R1zmo-zZrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAISS - wector embedding"
      ],
      "metadata": {
        "id": "3Ysl8URn0KD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "wrWBkeHzzZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"This is a document about machine learning.\",\n",
        "    \"Another document related to artificial intelligence.\",\n",
        "    \"More text about data science and machine learning.\",\n",
        "    \"Different topic about natural language processing.\",\n",
        "    \"Last document about deep learning.\"\n",
        "]\n",
        "\n",
        "# Generowanie osadzonych wektorów dla dokumentów\n",
        "embeddings = model.encode(documents)\n"
      ],
      "metadata": {
        "id": "pyxDukWC0Is4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)  # Używamy metryki L2 (odległość euklidesowa)\n",
        "\n",
        "# Sprawdzenie czy indeks jest pusto\n",
        "print(f\"Is index trained? {index.is_trained}\")\n"
      ],
      "metadata": {
        "id": "P54OzoT50Wxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sprawdzenie liczby wektorów w indeksie\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "jXCaTETX0WuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych dokumentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: {documents[idx]} (Distance: {distances[0][i]})\")\n"
      ],
      "metadata": {
        "id": "Em8lFs7n0Wrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zapisanie indeksu do pliku\n",
        "faiss.write_index(index, \"vector_index.faiss\")\n",
        "\n",
        "# Ładowanie indeksu z pliku\n",
        "index = faiss.read_index(\"vector_index.faiss\")\n",
        "\n",
        "# Sprawdzenie liczby wektorów w załadowanym indeksie\n",
        "print(f\"Number of vectors in the loaded index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "DxzPgLv80Wpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wector embeddings"
      ],
      "metadata": {
        "id": "YXlGxOwx-WAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "    return documents\n",
        "\n",
        "# Ścieżka do folderu z plikami tekstowymi\n",
        "folder_path = './documents'\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "# Generowanie osadzonych wektorów dla dokumentów\n",
        "embeddings = model.encode(documents)\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)  # Używamy metryki L2 (odległość euklidesowa)\n",
        "\n",
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sprawdzenie liczby wektorów w indeksie\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
        "\n",
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych dokumentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: {documents[idx]} (Distance: {distances[0][i]})\")\n",
        "\n",
        "# Zapisanie indeksu do pliku\n",
        "faiss.write_index(index, \"vector_index.faiss\")\n",
        "\n",
        "# Ładowanie indeksu z pliku\n",
        "index = faiss.read_index(\"vector_index.faiss\")\n",
        "\n",
        "# Sprawdzenie liczby wektorów w załadowanym indeksie\n",
        "print(f\"Number of vectors in the loaded index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "YX0mA9jL0WnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "    return documents\n",
        "\n",
        "def split_text_into_chunks(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings_for_chunks(documents, max_length=1000):\n",
        "    all_embeddings = []\n",
        "    for document in documents:\n",
        "        chunks = split_text_into_chunks(document, max_length)\n",
        "        embeddings = model.encode(chunks)\n",
        "        all_embeddings.extend(embeddings)\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "# Ścieżka do folderu z plikami tekstowymi\n",
        "folder_path = './documents'\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "# Generowanie osadzonych wektorów dla fragmentów\n",
        "embeddings = generate_embeddings_for_chunks(documents)\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)  # Używamy metryki L2 (odległość euklidesowa)\n",
        "\n",
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sprawdzenie liczby wektorów w indeksie\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
        "\n",
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych fragmentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: Fragment {idx} (Distance: {distances[0][i]})\")\n",
        "\n",
        "# Zapisanie indeksu do pliku\n",
        "faiss.write_index(index, \"vector_index.faiss\")\n",
        "\n",
        "# Ładowanie indeksu z pliku\n",
        "index = faiss.read_index(\"vector_index.faiss\")\n",
        "\n",
        "# Sprawdzenie liczby wektorów w załadowanym indeksie\n",
        "print(f\"Number of vectors in the loaded index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "Eb8_u4Br0Wk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "    return documents\n",
        "\n",
        "def split_text_into_chunks(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings_for_chunks(documents, max_length=1000):\n",
        "    all_embeddings = []\n",
        "    for document in documents:\n",
        "        chunks = split_text_into_chunks(document, max_length)\n",
        "        embeddings = model.encode(chunks)\n",
        "        all_embeddings.extend(embeddings)\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "# Ścieżka do folderu z plikami tekstowymi\n",
        "folder_path = 'path_to_your_folder/documents'\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "# Generowanie osadzonych wektorów dla fragmentów\n",
        "embeddings = generate_embeddings_for_chunks(documents)\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)  # Używamy metryki L2 (odległość euklidesowa)\n",
        "\n",
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sprawdzenie liczby wektorów w indeksie\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
        "\n",
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych fragmentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: Fragment {idx} (Distance: {distances[0][i]})\")\n",
        "\n",
        "# Zapisanie indeksu do pliku\n",
        "faiss.write_index(index, \"vector_index.faiss\")\n",
        "\n",
        "# Ładowanie indeksu z pliku\n",
        "index = faiss.read_index(\"vector_index.faiss\")\n",
        "\n",
        "# Sprawdzenie liczby wektorów w załadowanym indeksie\n",
        "print(f\"Number of vectors in the loaded index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "lOFPqbBoAFew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Ładowanie modelu SBERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "    return documents\n",
        "\n",
        "def split_text_into_chunks(text, max_length=1000):\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings_for_chunks(documents, max_length=1000):\n",
        "    all_embeddings = []\n",
        "    for document in documents:\n",
        "        chunks = split_text_into_chunks(document, max_length)\n",
        "        embeddings = model.encode(chunks)\n",
        "        all_embeddings.extend(embeddings)\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "# Ścieżka do folderu z plikami tekstowymi\n",
        "folder_path = 'path_to_your_folder/documents'\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "# Generowanie osadzonych wektorów dla fragmentów\n",
        "embeddings = generate_embeddings_for_chunks(documents)\n",
        "\n",
        "# Tworzenie indeksu FAISS\n",
        "dimension = embeddings.shape[1]  # Wymiar wektorów osadzonych\n",
        "index = faiss.IndexFlatL2(dimension)  # Używamy metryki L2 (odległość euklidesowa)\n",
        "\n",
        "# Dodawanie osadzonych wektorów do indeksu\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sprawdzenie liczby wektorów w indeksie\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
        "\n",
        "# Przykładowe zapytanie\n",
        "query = \"Information about AI and machine learning.\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Znalezienie 3 najbardziej podobnych fragmentów\n",
        "k = 3\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(f\"Query: {query}\")\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Result {i+1}: Fragment {idx} (Distance: {distances[0][i]})\")\n",
        "\n",
        "# Zapisanie indeksu do pliku\n",
        "faiss.write_index(index, \"vector_index.faiss\")\n",
        "\n",
        "# Ładowanie indeksu z pliku\n",
        "index = faiss.read_index(\"vector_index.faiss\")\n",
        "\n",
        "# Sprawdzenie liczby wektorów w załadowanym indeksie\n",
        "print(f\"Number of vectors in the loaded index: {index.ntotal}\")\n"
      ],
      "metadata": {
        "id": "Ts628zrkAFZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SRvZ5V8AFXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4FOhhHp9AFUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZMkdkeyAFR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z8Fg12nQAFIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}